{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# RLM Jupyter Integration: Working Documentation\n",
    "\n",
    "This notebook demonstrates the new RLM features for Jupyter environments:\n",
    "1. **Core Interfaces**: `completion()` vs `session.chat()`\n",
    "2. **Persistence**: Maintaining state across independent calls\n",
    "3. **Synchronization**: Sharing variables between the Notebook and RLM (One-way, Selective, and Bidirectional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from rlm import RLM\n",
    "from rlm.utils.notebook import render_completion\n",
    "from rlm.utils.trace_notebook import enable_trace_notebook\n",
    "\n",
    "# Optional: Enable trace logging to generate .trace.ipynb files for debugging\n",
    "logger, trace_handle = enable_trace_notebook(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-interfaces",
   "metadata": {},
   "source": [
    "## 1. Core Interfaces: Completions vs Sessions\n",
    "\n",
    "RLM distinguishes between *one-off tasks* and *conversational workflows*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsec-completion",
   "metadata": {},
   "source": [
    "### A. Completions (Stateless/One-off)\n",
    "Use `rlm.completion()` for independent tasks where history doesn't matter. By default, each call starts fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "completion-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1: random_number\n",
      "Result 2: 15\n"
     ]
    }
   ],
   "source": [
    "rlm = RLM(\n",
    "    environment=\"jupyter\",\n",
    "    logger=logger,\n",
    "    backend_kwargs={\"model_name\": \"gpt-4o\"}\n",
    ")\n",
    "\n",
    "# This call happens in isolation\n",
    "result = rlm.completion(\"Generate a random number between 1 and 100.\")\n",
    "print(f\"Result 1: {result.response}\")\n",
    "\n",
    "# This call knows nothing about the previous one\n",
    "result = rlm.completion(\"What was the number you just generated?\")\n",
    "print(f\"Result 2: {result.response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsec-session",
   "metadata": {},
   "source": [
    "### B. Sessions (Multi-turn/Conversational)\n",
    "Use `rlm.start_session()` for workflows where the model needs to see previous turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "session-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn 1: \"Blue\"\n",
      "Turn 2: \"I pick the color blue.\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Run Context\n",
       "```json\n",
       "{\n",
       "  \"prompt\": \"Pick a color.\",\n",
       "  \"root_prompt\": null\n",
       "}\n",
       "```\n",
       "## Iteration 1\n",
       "\n",
       "### Model Response\n",
       "\n",
       "To begin, I will inspect the contents of `session_context_0` to understand the prompt and the associated context so that I can determine the necessary steps to answer the query.\n",
       "\n",
       "## Iteration 2\n",
       "\n",
       "### Model Response\n",
       "\n",
       "Let's inspect the contents of `session_context_0` to understand what information it contains and what query needs to be answered.\n",
       "\n",
       "```repl\n",
       "print(session_context_0)\n",
       "```\n",
       "\n",
       "### Code Block 1\n",
       "\n",
       "```python\n",
       "print(session_context_0)\n",
       "```\n",
       "\n",
       "**stdout**\n",
       "\n",
       "```\n",
       "Pick a color.\n",
       "\n",
       "```\n",
       "\n",
       "## Iteration 3\n",
       "\n",
       "### Model Response\n",
       "\n",
       "The prompt \"Pick a color.\" is straightforward and doesn't require any complex analysis or additional context inspection. I will respond to the query directly.\n",
       "\n",
       "FINAL(\"Blue\")\n",
       "\n",
       "\n",
       "---\n",
       "\n",
       "## Run Context\n",
       "```json\n",
       "{\n",
       "  \"prompt\": \"Why did you pick that one?\",\n",
       "  \"root_prompt\": null\n",
       "}\n",
       "```\n",
       "## Iteration 1\n",
       "\n",
       "### Model Response\n",
       "\n",
       "The context in `session_context_0` simply contains the phrase \"Pick a color.\" This appears to be a prompt or directive but lacks additional context or a specific question to answer. \n",
       "\n",
       "Since there is no additional information or instructions included with \"Pick a color,\" it seems the task is to select a color based on this prompt.\n",
       "\n",
       "I will proceed with a response based on this context.\n",
       "\n",
       "FINAL(\"I pick the color blue.\")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "session = rlm.start_session()\n",
    "\n",
    "result = session.chat(\"Pick a color.\")\n",
    "print(f\"Turn 1: {result.response}\")\n",
    "\n",
    "result = session.chat(\"Why did you pick that one?\")\n",
    "print(f\"Turn 2: {result.response}\")\n",
    "\n",
    "result.render_trace() # View the cumulative trace of the session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-persistence",
   "metadata": {},
   "source": [
    "## 2. Persistence\n",
    "\n",
    "Sometimes you want the *REPL state* (variables defined by the model) to persist across completely separate `completion()` calls, even if you don't want to pass the full conversation history.\n",
    "\n",
    "Enable this with `persistent=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "persistent-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "persistent_rlm = RLM(\n",
    "    environment=\"jupyter\",\n",
    "    persistent=True,\n",
    "    logger=logger,\n",
    "    backend_kwargs={\"model_name\": \"gpt-5.2\"}\n",
    ")\n",
    "\n",
    "# Call 1: Define a variable in the RLM's environment\n",
    "persistent_rlm.completion(\"Set x = 500 in the repl\")\n",
    "\n",
    "# Call 2: Use that variable (even though this is a fresh completion call)\n",
    "result = persistent_rlm.completion(\"Read the value of x from the repl. do not assume the value. Then set x to the double of its value.\")\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-sync",
   "metadata": {},
   "source": [
    "## 3. Jupyter Synchronization\n",
    "\n",
    "The `jupyter` environment allows sharing variables between the Notebook kernel (where you are) and the RLM execution environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sync-1-title",
   "metadata": {},
   "source": [
    "### Mode A: One-Way Sync (RLM → Notebook)\n",
    "Useful when you want RLM to compute something and leave the result for you to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sync-out-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value from RLM: 83810205\n"
     ]
    }
   ],
   "source": [
    "rlm_out = RLM(\n",
    "    environment=\"jupyter\", \n",
    "    environment_kwargs={\"sync_to_user_ns\": True}, # Push RLM vars to Notebook\n",
    "    backend_kwargs={\"model_name\": \"gpt-4o\"}\n",
    ")\n",
    "\n",
    "rlm_out.completion(\"computed_value = 12345 * 6789\")\n",
    "\n",
    "# Now the variable is available in your notebook\n",
    "print(f\"Value from RLM: {computed_value}\") # noqa: F821"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sync-2-title",
   "metadata": {},
   "source": [
    "### Mode B: Selective Sync (Notebook → RLM)\n",
    "Useful when you have specific data (config, datasets) you want to securely pass to RLM, without exposing your entire namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sync-selective-demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"user_data is defined, but api_key is not defined.\"\n"
     ]
    }
   ],
   "source": [
    "# Define sensitive/heavy data in notebook\n",
    "api_key = \"sk-12345\"\n",
    "user_data = {\"id\": 1, \"name\": \"Petros\"}\n",
    "irrelevant_var = \"Do not see this\"\n",
    "\n",
    "rlm_dev = RLM(\n",
    "    environment=\"jupyter\",\n",
    "    environment_kwargs={\n",
    "        \"sync_from_user_ns\": True,\n",
    "        \"sync_vars\": [\"user_data\"] # Only sync this variable\n",
    "    },\n",
    "    backend_kwargs={\"model_name\": \"gpt-4o\"}\n",
    ")\n",
    "\n",
    "result = rlm_dev.completion(\"Check if 'user_data' is defined. Check if 'api_key' is defined.\")\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sync-3-title",
   "metadata": {},
   "source": [
    "### Mode C: Full Bidirectional Sync (Zero-Copy)\n",
    "The \"Co-pilot\" mode. RLM shares your exact karnel state. It can read your variables, modify them, and create new ones. \n",
    "**Note:** No serialization is performed; objects are passed by reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sync-full-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "  Name  Score\n",
      "0    A     10\n",
      "1    B     20\n"
     ]
    }
   ],
   "source": [
    "# Setup data\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\"Name\": [\"A\", \"B\"], \"Score\": [10, 20]})\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sync-full-exec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modified DataFrame (in Notebook):\n",
      "  Name  Score\n",
      "0    A     15\n",
      "1    B     25\n"
     ]
    }
   ],
   "source": [
    "rlm_full = RLM(\n",
    "    environment=\"jupyter\",\n",
    "    environment_kwargs={\n",
    "        \"sync_to_user_ns\": True,\n",
    "        \"sync_from_user_ns\": True\n",
    "        # No 'sync_vars' means sync everything\n",
    "    },\n",
    "    backend_kwargs={\"model_name\": \"gpt-5.2\"}\n",
    ")\n",
    "\n",
    "# RLM modifies 'df' in place\n",
    "rlm_full.completion(\"Add 5 to the Score column of df.\")\n",
    "\n",
    "print(\"\\nModified DataFrame (in Notebook):\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlm-venv",
   "language": "python",
   "name": "rlm-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "rlm_trace_enabled": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
